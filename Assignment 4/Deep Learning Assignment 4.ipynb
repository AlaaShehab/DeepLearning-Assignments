{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep Learning Assignment 4.ipynb","provenance":[],"collapsed_sections":["4kfQz7A9dY_C"],"authorship_tag":"ABX9TyNLSUZRZrX7cgQLVzBQ4Dh5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4kfQz7A9dY_C","colab_type":"text"},"source":["# **PART I**"]},{"cell_type":"code","metadata":{"id":"OI4KURBd3lbJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum(axis=0)\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","\n","def initialize_adam(parameters) :\n","    \"\"\"\n","    Initializes v and s as two python dictionaries with:\n","                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n","                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters.\n","                    parameters[\"W\" + str(l)] = Wl\n","                    parameters[\"b\" + str(l)] = bl\n","    \n","    Returns: \n","    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n","                    v[\"dW\" + str(l)] = ...\n","                    v[\"db\" + str(l)] = ...\n","    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n","                    s[\"dW\" + str(l)] = ...\n","                    s[\"db\" + str(l)] = ...\n","\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural networks\n","    v = {}\n","    s = {}\n","    \n","    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n","    for l in range(L):\n","    ### START CODE HERE ### (approx. 4 lines)\n","        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n","        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n","        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n","        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n","    ### END CODE HERE ###\n","    \n","    return v, s\n","\n","\n","def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n","                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n","    \"\"\"\n","    Update parameters using Adam\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters:\n","                    parameters['W' + str(l)] = Wl\n","                    parameters['b' + str(l)] = bl\n","    grads -- python dictionary containing your gradients for each parameters:\n","                    grads['dW' + str(l)] = dWl\n","                    grads['db' + str(l)] = dbl\n","    v -- Adam variable, moving average of the first gradient, python dictionary\n","    s -- Adam variable, moving average of the squared gradient, python dictionary\n","    learning_rate -- the learning rate, scalar.\n","    beta1 -- Exponential decay hyperparameter for the first moment estimates \n","    beta2 -- Exponential decay hyperparameter for the second moment estimates \n","    epsilon -- hyperparameter preventing division by zero in Adam updates\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","    v -- Adam variable, moving average of the first gradient, python dictionary\n","    s -- Adam variable, moving average of the squared gradient, python dictionary\n","    \"\"\"\n","    \n","    L = len(parameters) // 2                 # number of layers in the neural networks\n","    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n","    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n","    \n","    # Perform Adam update on all parameters\n","    for l in range(L):\n","        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)] \n","        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)] \n","        ### END CODE HERE ###\n","\n","        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n","        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n","        ### END CODE HERE ###\n","\n","        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n","        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n","        ### END CODE HERE ###\n","\n","        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n","        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n","        ### END CODE HERE ###\n","\n","        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n","        ### START CODE HERE ### (approx. 2 lines)\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n","        ### END CODE HERE ###\n","\n","    return parameters, v, s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQwBlkyG0XPv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":890},"outputId":"f7aeef20-4b22-4aaa-a9b7-1200404622c5","executionInfo":{"status":"ok","timestamp":1586307766809,"user_tz":-120,"elapsed":79074,"user":{"displayName":"alaa shehab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZ8jCzZ-_fmLm76_wkQYTlLHiQQeUdFSy2mrAh7A=s64","userId":"06392076481225704354"}}},"source":["import numpy as np\n","\n","def rnn_cell_forward(xt, a_prev, parameters):\n","    \"\"\"\n","    Implements a single forward step of the RNN-cell as described in Figure (2)\n","\n","    Arguments:\n","    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n","    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n","    parameters -- python dictionary containing:\n","                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n","                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n","                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        ba --  Bias, numpy array of shape (n_a, 1)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","    Returns:\n","    a_next -- next hidden state, of shape (n_a, m)\n","    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n","    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n","    \"\"\"\n","    \n","    # Retrieve parameters from \"parameters\"\n","    Wax = parameters[\"Wax\"]\n","    Waa = parameters[\"Waa\"]\n","    Wya = parameters[\"Wya\"]\n","    ba = parameters[\"ba\"]\n","    by = parameters[\"by\"]\n","    \n","    ### START CODE HERE ### \n","    # compute next activation state using the formula given above\n","    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n","    # compute output of the current cell using the formula given above\n","    yt_pred = softmax(np.dot(Wya, a_next) + by)\n","    ### END CODE HERE ###\n","    \n","    # store values you need for backward propagation in cache\n","    cache = (a_next, a_prev, xt, parameters)\n","    \n","    return a_next, yt_pred, cache\n","  \n","def rnn_forward(x, a0, parameters):\n","    \"\"\"\n","    Implement the forward propagation of the recurrent neural network described in Figure (3).\n","\n","    Arguments:\n","    x -- Input data for every time-step, of shape (n_x, m, T_x).\n","    a0 -- Initial hidden state, of shape (n_a, m)\n","    parameters -- python dictionary containing:\n","                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n","                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n","                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        ba --  Bias numpy array of shape (n_a, 1)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","\n","    Returns:\n","    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n","    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n","    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n","    \"\"\"\n","    \n","    # Initialize \"caches\" which will contain the list of all caches\n","    caches = []\n","    \n","    # Retrieve dimensions from shapes of x and Wy\n","    n_x, m, T_x = x.shape\n","    n_y, n_a = parameters[\"Wya\"].shape\n","    \n","    ### START CODE HERE ###\n","    \n","    # initialize \"a\" and \"y\" with zeros\n","    a = np.zeros([n_a, m, T_x])\n","    y_pred = np.zeros([n_y, m, T_x])\n","\n","    # Initialize a_next\n","    a_next = a0\n","\n","    # loop over all time-steps\n","    for i in range(T_x):\n","        # Update next hidden state, compute the prediction, get the cache\n","        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,i], a_next, parameters)\n","        # Save the value of the new \"next\" hidden state in a\n","        a[:,:,i] = a_next\n","        # Save the value of the prediction in y\n","        y_pred[:,:,i] = yt_pred\n","        # Append \"cache\" to \"caches\"\n","        caches.append(cache) \n","    ### END CODE HERE ###\n","    \n","    # store values needed for backward propagation in cache\n","    caches = (caches, x)\n","    \n","    return a, y_pred, caches\n","  \n","  \n","# GRADED FUNCTION: lstm_cell_forward\n","\n","def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n","    \"\"\"\n","    Implement a single forward step of the LSTM-cell as described in Figure (4)\n","\n","    Arguments:\n","    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n","    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n","    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n","    parameters -- python dictionary containing:\n","                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n","                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n","                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n","                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n","                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n","                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n","                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n","                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n","                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","                        \n","    Returns:\n","    a_next -- next hidden state, of shape (n_a, m)\n","    c_next -- next memory state, of shape (n_a, m)\n","    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n","    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n","    \n","    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n","          c stands for the memory value\n","    \"\"\"\n","\n","    # Retrieve parameters from \"parameters\"\n","    Wf = parameters[\"Wf\"]\n","    bf = parameters[\"bf\"]\n","    Wi = parameters[\"Wi\"]\n","    bi = parameters[\"bi\"]\n","    Wc = parameters[\"Wc\"]\n","    bc = parameters[\"bc\"]\n","    Wo = parameters[\"Wo\"]\n","    bo = parameters[\"bo\"]\n","    Wy = parameters[\"Wy\"]\n","    by = parameters[\"by\"]\n","    \n","    # Retrieve dimensions from shapes of xt and Wy\n","    n_x, m = xt.shape\n","    n_y, n_a = Wy.shape\n","\n","    ### START CODE HERE ###\n","    # Concatenate a_prev and xt \n","    concat = np.concatenate((a_prev, xt), axis=0)\n","    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4)\n","    ft = sigmoid(np.dot(Wf, concat) + bf)\n","    it = sigmoid(np.dot(Wi, concat) + bi)\n","    ot = sigmoid(np.dot(Wo, concat) + bo)\n","\n","    cct = np.tanh(np.dot(Wc, concat) + bc)\n","    c_next = ft*c_prev + it*cct\n","    a_next = ot * np.tanh(c_next)\n","\n","    # Compute prediction of the LSTM cell \n","    yt_pred = softmax(np.dot(Wy, a_next) + by)\n","    ### END CODE HERE ###\n","\n","    # store values needed for backward propagation in cache\n","    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n","\n","    return a_next, c_next, yt_pred, cache\n","  \n","# GRADED FUNCTION: lstm_forward\n","\n","def lstm_forward(x, a0, parameters):\n","    \"\"\"\n","    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).\n","\n","    Arguments:\n","    x -- Input data for every time-step, of shape (n_x, m, T_x).\n","    a0 -- Initial hidden state, of shape (n_a, m)\n","    parameters -- python dictionary containing:\n","                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n","                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n","                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n","                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n","                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n","                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n","                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n","                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n","                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","                        \n","    Returns:\n","    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n","    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n","    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n","    \"\"\"\n","\n","    # Initialize \"caches\", which will track the list of all the caches\n","    caches = []\n","    \n","    ### START CODE HERE ###\n","    # Retrieve dimensions from shapes of x and Wy \n","    n_x, m, T_x = x.shape\n","    n_y, n_a = parameters[\"Wy\"].shape\n","    \n","    # initialize \"a\", \"c\" and \"y\" with zeros\n","    a = np.zeros([n_a, m, T_x])\n","    y = np.zeros([n_y, m, T_x])\n","    c = np.zeros([n_a, m, T_x])\n","\n","    # Initialize a_next and c_next\n","    a_next = a0\n","    c_next = np.zeros([n_a,m])\n","\n","    for i in range(T_x):\n","      # Update next hidden state, compute the prediction, get the cache\n","      a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:,:,i], a_next, c_next, parameters)\n","      # Save the value of the new \"next\" hidden state in a\n","      a[:,:,i] = a_next\n","      # Save the value of the prediction in y\n","      y[:,:,i] = yt_pred\n","      # Save the value of the next cell state\n","      c[:,:,i]  = c_next\n","      # Append \"cache\" to \"caches\"\n","      caches.append(cache) \n","        \n","    ### END CODE HERE ###\n","    \n","    # store values needed for backward propagation in cache\n","    caches = (caches, x)\n","\n","    return a, y, c, caches\n","  \n","def rnn_cell_backward(da_next, cache):\n","    \"\"\"\n","    Implements the backward pass for the RNN-cell (single time-step).\n","\n","    Arguments:\n","    da_next -- Gradient of loss with respect to next hidden state\n","    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n","\n","    Returns:\n","    gradients -- python dictionary containing:\n","                        dx -- Gradients of input data, of shape (n_x, m)\n","                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n","                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n","                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n","                        dba -- Gradients of bias vector, of shape (n_a, 1)\n","    \"\"\"\n","    \n","    # Retrieve values from cache\n","    (a_next, a_prev, xt, parameters) = cache\n","    \n","    # Retrieve values from parameters\n","    Wax = parameters[\"Wax\"]\n","    Waa = parameters[\"Waa\"]\n","    Wya = parameters[\"Wya\"]\n","    ba = parameters[\"ba\"]\n","    by = parameters[\"by\"]\n","\n","    ### START CODE HERE ###\n","    dtanh = (1 - (a_next)**2) * da_next\n","    da_prev = np.dot(Waa.T, dtanh)\n","    dxt = np.dot(Wax.T, dtanh)\n","    dWax = np.dot(dtanh, xt.T)\n","    dWaa = np.dot(dtanh, a_prev.T)\n","    dba = np.sum(dtanh, axis=-1, keepdims=True)\n","\n","    ### END CODE HERE ###\n","    \n","    # Store the gradients in a python dictionary\n","    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n","    \n","    return gradients\n","  \n","def rnn_backward(da, caches):\n","    \"\"\"\n","    Implement the backward pass for a RNN over an entire sequence of input data.\n","\n","    Arguments:\n","    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n","    caches -- tuple containing information from the forward pass (rnn_forward)\n","    \n","    Returns:\n","    gradients -- python dictionary containing:\n","                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n","                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n","                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n","                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)\n","                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n","    \"\"\"\n","        \n","    ### START CODE HERE ###\n","    \n","    # Retrieve values from the first cache (t=1) of caches\n","    (caches, x) = caches\n","    (a_next, a_prev, xt, parameters) = caches[0]\n","\n","    # Retrieve dimensions from da's and x1's shapes \n","    n_x, m, T_x = x.shape\n","    n_a, m, T_x = da.shape\n","\n","    # initialize the gradients with the right sizes \n","    dx = np.zeros([n_x, m, T_x])\n","    da0 = np.zeros([n_a, m])\n","    da_prev = np.zeros([n_a, m])\n","    dWax = np.zeros([n_a, n_x])\n","    dWaa = np.zeros([n_a, n_a])\n","    dba = np.zeros([n_a, 1])\n","\n","    # Loop through all the time steps\n","    for i in reversed(range(T_x)):\n","        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step.\n","        gradients = rnn_cell_backward(da_prev + da[:,:,i], caches[i])\n","        # Retrieve derivatives from gradients\n","        dxt = gradients[\"dxt\"]\n","        da_prev = gradients[\"da_prev\"]\n","        dWaxt = gradients[\"dWax\"]\n","        dWaat = gradients[\"dWaa\"]\n","        dbat = gradients[\"dba\"]\n","        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t\n","        dba += dbat\n","        dWaa += dWaat\n","        dWax += dWaxt\n","        dx[:,:,i] = dxt\n","\n","    # Set da0 to the gradient of a which has been backpropagated through all time-steps \n","    da0 = da_prev\n","    ### END CODE HERE ###\n","\n","    # Store the gradients in a python dictionary\n","    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n","    \n","    return gradients\n","  \n","np.random.seed(1)\n","xt = np.random.randn(3,10)\n","a_prev = np.random.randn(5,10)\n","Waa = np.random.randn(5,5)\n","Wax = np.random.randn(5,3)\n","Wya = np.random.randn(2,5)\n","ba = np.random.randn(5,1)\n","by = np.random.randn(2,1)\n","parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n","\n","a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n","print(\"a_next[4] = \", a_next[4])\n","print(\"a_next.shape = \", a_next.shape)\n","print(\"yt_pred[1] =\", yt_pred[1])\n","print(\"yt_pred.shape = \", yt_pred.shape)\n","\n","\n","np.random.seed(1)\n","x = np.random.randn(3,10,4)\n","a0 = np.random.randn(5,10)\n","Waa = np.random.randn(5,5)\n","Wax = np.random.randn(5,3)\n","Wya = np.random.randn(2,5)\n","ba = np.random.randn(5,1)\n","by = np.random.randn(2,1)\n","parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n","\n","a, y_pred, caches = rnn_forward(x, a0, parameters)\n","print(\"a[4][1] = \", a[4][1])\n","print(\"a.shape = \", a.shape)\n","print(\"y_pred[1][3] =\", y_pred[1][3])\n","print(\"y_pred.shape = \", y_pred.shape)\n","print(\"caches[1][1][3] =\", caches[1][1][3])\n","print(\"len(caches) = \", len(caches))\n","\n","np.random.seed(1)\n","xt = np.random.randn(3,10)\n","a_prev = np.random.randn(5,10)\n","c_prev = np.random.randn(5,10)\n","Wf = np.random.randn(5, 5+3)\n","bf = np.random.randn(5,1)\n","Wi = np.random.randn(5, 5+3)\n","bi = np.random.randn(5,1)\n","Wo = np.random.randn(5, 5+3)\n","bo = np.random.randn(5,1)\n","Wc = np.random.randn(5, 5+3)\n","bc = np.random.randn(5,1)\n","Wy = np.random.randn(2,5)\n","by = np.random.randn(2,1)\n","\n","parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n","\n","a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n","print(\"a_next[4] = \", a_next[4])\n","print(\"a_next.shape = \", c_next.shape)\n","print(\"c_next[2] = \", c_next[2])\n","print(\"c_next.shape = \", c_next.shape)\n","print(\"yt[1] =\", yt[1])\n","print(\"yt.shape = \", yt.shape)\n","print(\"cache[1][3] =\", cache[1][3])\n","print(\"len(cache) = \", len(cache))\n","\n","np.random.seed(1)\n","x = np.random.randn(3,10,7)\n","a0 = np.random.randn(5,10)\n","Wf = np.random.randn(5, 5+3)\n","bf = np.random.randn(5,1)\n","Wi = np.random.randn(5, 5+3)\n","bi = np.random.randn(5,1)\n","Wo = np.random.randn(5, 5+3)\n","bo = np.random.randn(5,1)\n","Wc = np.random.randn(5, 5+3)\n","bc = np.random.randn(5,1)\n","Wy = np.random.randn(2,5)\n","by = np.random.randn(2,1)\n","\n","parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n","\n","a, y, c, caches = lstm_forward(x, a0, parameters)\n","print(\"a[4][3][6] = \", a[4][3][6])\n","print(\"a.shape = \", a.shape)\n","print(\"y[1][4][3] =\", y[1][4][3])\n","print(\"y.shape = \", y.shape)\n","print(\"caches[1][1[1]] =\", caches[1][1][1])\n","print(\"c[1][2][1]\", c[1][2][1])\n","print(\"len(caches) = \", len(caches))\n","\n","np.random.seed(1)\n","xt = np.random.randn(3,10)\n","a_prev = np.random.randn(5,10)\n","Wax = np.random.randn(5,3)\n","Waa = np.random.randn(5,5)\n","Wya = np.random.randn(2,5)\n","b = np.random.randn(5,1)\n","by = np.random.randn(2,1)\n","parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n","\n","a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n","\n","da_next = np.random.randn(5,10)\n","gradients = rnn_cell_backward(da_next, cache)\n","print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n","print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n","print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n","print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n","print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n","print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n","print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n","print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n","print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n","print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)\n","\n","np.random.seed(1)\n","x = np.random.randn(3,10,4)\n","a0 = np.random.randn(5,10)\n","Wax = np.random.randn(5,3)\n","Waa = np.random.randn(5,5)\n","Wya = np.random.randn(2,5)\n","ba = np.random.randn(5,1)\n","by = np.random.randn(2,1)\n","parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n","a, y, caches = rnn_forward(x, a0, parameters)\n","da = np.random.randn(5, 10, 4)\n","gradients = rnn_backward(da, caches)\n","\n","print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n","print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n","print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n","print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n","print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n","print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n","print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n","print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n","print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n","print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n"," -0.18887155  0.99815551  0.6531151   0.82872037]\n","a_next.shape =  (5, 10)\n","yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n"," 0.36920224 0.9966312  0.9982559  0.17746526]\n","yt_pred.shape =  (2, 10)\n","a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n","a.shape =  (5, 10, 4)\n","y_pred[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n","y_pred.shape =  (2, 10, 4)\n","caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n","len(caches) =  2\n","a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n","  0.76566531  0.34631421 -0.00215674  0.43827275]\n","a_next.shape =  (5, 10)\n","c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n","  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n","c_next.shape =  (5, 10)\n","yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n"," 0.00943007 0.12666353 0.39380172 0.07828381]\n","yt.shape =  (2, 10)\n","cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n","  0.07651101 -1.03752894  1.41219977 -0.37647422]\n","len(cache) =  10\n","a[4][3][6] =  0.17211776753291672\n","a.shape =  (5, 10, 7)\n","y[1][4][3] = 0.9508734618501101\n","y.shape =  (2, 10, 7)\n","caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n","  0.41005165]\n","c[1][2][1] -0.8555449167181981\n","len(caches) =  2\n","gradients[\"dxt\"][1][2] = -0.4605641030588796\n","gradients[\"dxt\"].shape = (3, 10)\n","gradients[\"da_prev\"][2][3] = 0.08429686538067724\n","gradients[\"da_prev\"].shape = (5, 10)\n","gradients[\"dWax\"][3][1] = 0.39308187392193034\n","gradients[\"dWax\"].shape = (5, 3)\n","gradients[\"dWaa\"][1][2] = -0.28483955786960663\n","gradients[\"dWaa\"].shape = (5, 5)\n","gradients[\"dba\"][4] = [0.80517166]\n","gradients[\"dba\"].shape = (5, 1)\n","gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\n","gradients[\"dx\"].shape = (3, 10, 4)\n","gradients[\"da0\"][2][3] = -0.31494237512664996\n","gradients[\"da0\"].shape = (5, 10)\n","gradients[\"dWax\"][3][1] = 11.264104496527777\n","gradients[\"dWax\"].shape = (5, 3)\n","gradients[\"dWaa\"][1][2] = 2.303333126579893\n","gradients[\"dWaa\"].shape = (5, 5)\n","gradients[\"dba\"][4] = [-0.74747722]\n","gradients[\"dba\"].shape = (5, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FyQrjVP1dd-u","colab_type":"text"},"source":["# **BONUS**"]},{"cell_type":"code","metadata":{"id":"J7cia0gNeCy5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"b7c28dd8-c790-4e80-f499-85c79faf13b4","executionInfo":{"status":"ok","timestamp":1586304224282,"user_tz":-120,"elapsed":7698,"user":{"displayName":"alaa shehab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZ8jCzZ-_fmLm76_wkQYTlLHiQQeUdFSy2mrAh7A=s64","userId":"06392076481225704354"}}},"source":["%tensorflow_version 1.14\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import GRU\n","from keras.layers import SimpleRNN\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","\n","vocab = 5000\n","max_words = 500\n","epochs = 3\n","batch_size=64\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab)\n","x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n","x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n","\n","print(x_train.shape)\n","print(x_test.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.14`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow is already loaded. Please restart the runtime to change versions.\n","(25000, 500)\n","(25000, 500)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NKzUcAQU3oJ2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"outputId":"ef571ab6-b9ea-4096-e130-5c15e80283b3","executionInfo":{"status":"ok","timestamp":1586307765349,"user_tz":-120,"elapsed":2921801,"user":{"displayName":"alaa shehab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZ8jCzZ-_fmLm76_wkQYTlLHiQQeUdFSy2mrAh7A=s64","userId":"06392076481225704354"}}},"source":["# Model 1\n","model = Sequential()\n","model.add(Embedding(vocab, 128, input_length=max_words))\n","model.add(LSTM(128))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2)\n","\n","scores = model.evaluate(x_test, y_test, verbose=0)\n","print(\"LSTM Accuracy: %.2f%%\" % (scores[1]*100))\n","# LSTM Accuracy = 87.2%"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/3\n"," - 438s - loss: 0.5708 - acc: 0.7128 - val_loss: 0.4016 - val_acc: 0.8242\n","Epoch 2/3\n"," - 423s - loss: 0.3420 - acc: 0.8583 - val_loss: 0.3550 - val_acc: 0.8482\n","Epoch 3/3\n"," - 433s - loss: 0.2542 - acc: 0.9018 - val_loss: 0.3117 - val_acc: 0.8720\n","LSTM Accuracy: 87.20%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"byShpI5_3o2E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"outputId":"fec6b50f-1ec5-49b3-ea94-b2ed3da2dc58","executionInfo":{"status":"ok","timestamp":1586306253688,"user_tz":-120,"elapsed":1412496,"user":{"displayName":"alaa shehab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZ8jCzZ-_fmLm76_wkQYTlLHiQQeUdFSy2mrAh7A=s64","userId":"06392076481225704354"}}},"source":["# Model 2\n","model = Sequential()\n","model.add(Embedding(vocab, 128, input_length=max_words))\n","model.add(GRU(128))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2)\n","\n","scores = model.evaluate(x_test, y_test, verbose=0)\n","print(\"GRU Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","# GRU Accuracy = 89.19%"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/3\n"," - 364s - loss: 0.4852 - acc: 0.7608 - val_loss: 0.8181 - val_acc: 0.6265\n","Epoch 2/3\n"," - 363s - loss: 0.3340 - acc: 0.8601 - val_loss: 0.3342 - val_acc: 0.8681\n","Epoch 3/3\n"," - 360s - loss: 0.2333 - acc: 0.9084 - val_loss: 0.2769 - val_acc: 0.8919\n","GRU Accuracy: 89.19%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h0Hl3uxL4KMT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"outputId":"a9f90b35-cc75-4c4c-c0b2-2e458d60f03e","executionInfo":{"status":"ok","timestamp":1586304975494,"user_tz":-120,"elapsed":743334,"user":{"displayName":"alaa shehab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZ8jCzZ-_fmLm76_wkQYTlLHiQQeUdFSy2mrAh7A=s64","userId":"06392076481225704354"}}},"source":["# Model 3\n","model = Sequential()\n","model.add(Embedding(vocab, 128, input_length=max_words))\n","model.add(SimpleRNN(128))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, verbose=2)\n","\n","scores = model.evaluate(x_test, y_test, verbose=0)\n","print(\"Simple RNN Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","# Simple RNN Accuracy: 63.40%"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/3\n"," - 197s - loss: 0.6513 - acc: 0.5977 - val_loss: 0.6449 - val_acc: 0.6247\n","Epoch 2/3\n"," - 193s - loss: 0.6073 - acc: 0.6659 - val_loss: 0.6421 - val_acc: 0.6096\n","Epoch 3/3\n"," - 189s - loss: 0.5911 - acc: 0.6766 - val_loss: 0.6257 - val_acc: 0.6340\n","Simple RNN Accuracy: 63.40%\n"],"name":"stdout"}]}]}